{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "CIBbdz6ew7CZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# ℓ1 and ℓ2 Regularization\n",
        "\n",
        "for simple linear models, you can use ℓ1 and ℓ2 regularization\n",
        "to constrain a neural network’s connection weights\n",
        "\n",
        "One way to do this using TensorFlow is to simply add the appropriate regularization\n",
        "terms to your cost function. For example, assuming you have just one hidden layer\n",
        "with weights weights1 and one output layer with weights weights2, then you can\n",
        "apply ℓ1 regularization like this:"
      ]
    },
    {
      "metadata": {
        "id": "ymYJPpgswxl3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "n_inputs = 28 * 28  # MNIST\n",
        "n_hidden1 = 300\n",
        "n_hidden2 = 50\n",
        "n_outputs = 10\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
        "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
        "\n",
        "#\n",
        "# Implementation of regularization \n",
        "#\n",
        "scale = 0.001\n",
        "\n",
        "my_dense_layer = partial(\n",
        "    tf.layers.dense, activation=tf.nn.relu,\n",
        "    kernel_regularizer=tf.contrib.layers.l1_regularizer(scale))\n",
        "\n",
        "with tf.name_scope(\"dnn\"):\n",
        "    hidden1 = my_dense_layer(X, n_hidden1, name=\"hidden1\")\n",
        "    hidden2 = my_dense_layer(hidden1, n_hidden2, name=\"hidden2\")\n",
        "    logits = my_dense_layer(hidden2, n_outputs, activation=None,name=\"outputs\")\n",
        "\n",
        "with tf.name_scope(\"loss\"):                                   \n",
        "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)                                \n",
        "    base_loss = tf.reduce_mean(xentropy, name=\"avg_xentropy\")   \n",
        "    reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
        "    loss = tf.add_n([base_loss] + reg_losses, name=\"loss\")    \n",
        "    \n",
        "\n",
        "#\n",
        "# Rest is normal \n",
        "#\n",
        "with tf.name_scope(\"eval\"):\n",
        "    correct = tf.nn.in_top_k(logits, y, 1)\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
        "\n",
        "learning_rate = 0.01\n",
        "\n",
        "with tf.name_scope(\"train\"):\n",
        "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
        "    training_op = optimizer.minimize(loss)\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "saver = tf.train.Saver()    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5SQ_pheZwxpP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "n_epochsn_epochs = 20\n",
        "batch_size = 200\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    init.run()\n",
        "    for epoch in range(n_epochs):\n",
        "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
        "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
        "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
        "        print(epoch, \"Validation accuracy:\", accuracy_val)\n",
        "\n",
        "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "a5DTAlbDo_b4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This code creates a neural network with two hidden layers and one output layer, and\n",
        "it also creates nodes in the graph to compute the ℓ1 regularization loss corresponding\n",
        "to each layer’s weights. TensorFlow automatically adds these nodes to a special collection\n",
        "containing all the regularization losses. You just need to add these regularization\n",
        "losses to your overall loss, like this:\n",
        "            \n",
        "            reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
        "            loss = tf.add_n([base_loss] + reg_losses, name=\"loss\")\n",
        "            \n",
        "  \n",
        "  \n",
        " # Dropout \n",
        " \n",
        " It is a fairly simple algorithm: at every training step, every neuron (including the\n",
        "input neurons but excluding the output neurons) has a **probability p** of being temporarily\n",
        "“dropped out,” meaning it will be entirely ignored during this training step,\n",
        "but it may be active during the next step. The hyperparameter p is\n",
        "called the dropout rate, and it is typically set to 50%. After training, neurons don’t get\n",
        "dropped anymore.\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/644/1*dEi_IkVB7IpkzZ-6H0Vpsg.png\">\n",
        "\n",
        "\n",
        "Neurons trained with dropout cannot co-adapt with their neighboring neurons; they have to be as useful as possible on their own. They also cannot rely excessively on just a few input neurons; they must pay attention to each of their input neurons. They end up being less sensitive to slight\n",
        "changes in the inputs. In the end you get a more robust network that generalizes better.\n",
        "\n",
        "There is one small but important technical detail. Suppose p = 50, in which case during\n",
        "testing a neuron will be connected to twice as many input neurons as it was (on\n",
        "average) during training. To compensate for this fact, we need to multiply each neuron\n",
        "input connection weights by 0.5 after training. If we don’t, each neuron will get a\n",
        "total input signal roughly twice as large as what the network was trained on, and it is\n",
        "unlikely to perform well. More generally, we need to multiply each input connection\n",
        "weight by the keep probability (1 – p) after training. Alternatively, we can divide each\n",
        "neuron’s output by the keep probability during training (these alternatives are not\n",
        "perfectly equivalent, but they work equally well).\n",
        "\n",
        "An example of the implementation of the dropout regularization is the next code:\n",
        "\n",
        "    "
      ]
    },
    {
      "metadata": {
        "id": "nfnjbmD9pVBd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
        "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
        "\n",
        "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
        "\n",
        "dropout_rate = 0.5  # == 1 - keep_prob\n",
        "X_drop = tf.layers.dropout(X, dropout_rate, training=training)\n",
        "\n",
        "with tf.name_scope(\"dnn\"):\n",
        "    hidden1 = tf.layers.dense(X_drop, n_hidden1, activation=tf.nn.relu)\n",
        "    hidden1_drop = tf.layers.dropout(hidden1, dropout_rate, training=training)\n",
        "    hidden2 = tf.layers.dense(hidden1_drop, n_hidden2, activation=tf.nn.relu)\n",
        "    hidden2_drop = tf.layers.dropout(hidden2, dropout_rate, training=training)\n",
        "    logits = tf.layers.dense(hidden2_drop, n_outputs)\n",
        "    \n",
        "    \n",
        "with tf.name_scope(\"loss\"):\n",
        "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
        "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
        "\n",
        "with tf.name_scope(\"train\"):\n",
        "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=0.9)\n",
        "    training_op = optimizer.minimize(loss)    \n",
        "\n",
        "with tf.name_scope(\"eval\"):\n",
        "    correct = tf.nn.in_top_k(logits, y, 1)\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
        "    \n",
        "init = tf.global_variables_initializer()\n",
        "saver = tf.train.Saver()\n",
        "\n",
        "\n",
        "n_epochs = 20\n",
        "batch_size = 50\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    init.run()\n",
        "    for epoch in range(n_epochs):\n",
        "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
        "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch, training: True})\n",
        "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
        "        print(epoch, \"Validation accuracy:\", accuracy_val)\n",
        "\n",
        "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nA4qFaIV5xLl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "     \n",
        "*You want to use the dropout() function in tensorflow.con\n",
        "trib.layers, not the one in tensorflow.nn. The first one turns off\n",
        "(no-op) when not training, which is what you want, while the second\n",
        "one does not.*\n",
        "\n",
        "Of course, just like you did earlier for Batch Normalization, you need to set is_train\n",
        "ing to True when training, and to False when testing.\n",
        "\n",
        "If you observe that the model is overfitting, you can increase the dropout rate (i.e.,\n",
        "reduce the keep_prob hyperparameter). Conversely, you should try decreasing the\n",
        "dropout rate (i.e., increasing keep_prob) if the model underfits the training set. It can\n",
        "also help to increase the dropout rate for large layers, and reduce it for small ones.\n",
        "\n",
        "Dropout does tend to significantly slow down convergence, but it usually results in a\n",
        "much better model when tuned properly. So, it is generally well worth the extra time\n",
        "and effort."
      ]
    },
    {
      "metadata": {
        "id": "rDcAyPAE6ySC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rMEJRDo07eIp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Max-Norm Regularization\n",
        "\n",
        "Another regularization technique that is quite popular for neural networks is called\n",
        "max-norm regularization: for each neuron, it constrains the weights **w** of the incoming\n",
        "connections such that ∥ w ∥2 ≤ r, where r is the max-norm hyperparameter and\n",
        "∥ · ∥2 is the ℓ2 norm.\n",
        "\n",
        "Reducing r increases the amount of regularization and helps reduce overfitting. Maxnorm\n",
        "regularization can also help alleviate the vanishing/exploding gradients problems\n",
        "(if you are not using Batch Normalization).\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "g1-0gs3r7kgf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "362b25ae-0510-442f-d96c-fff6b772aa6d"
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "n_inputs = 28 * 28\n",
        "n_hidden1 = 300\n",
        "n_hidden2 = 50\n",
        "n_outputs = 10\n",
        "\n",
        "learning_rate = 0.01\n",
        "momentum = 0.9\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
        "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
        "\n",
        "def max_norm_regularizer(threshold, axes=1, name=\"max_norm\",\n",
        "                         collection=\"max_norm\"):\n",
        "    def max_norm(weights):\n",
        "        clipped = tf.clip_by_norm(weights, clip_norm=threshold, axes=axes)\n",
        "        clip_weights = tf.assign(weights, clipped, name=name)\n",
        "        tf.add_to_collection(collection, clip_weights)\n",
        "        return None # there is no regularization loss term\n",
        "    return max_norm\n",
        "  \n",
        "  \n",
        "# Then you can call this function to get a max norm regularizer\n",
        "# (with the threshold you want). When you create a hidden layer, you can pass this regularizer to the kernel_regularizer argument:  \n",
        "\n",
        "\n",
        "max_norm_reg = max_norm_regularizer(threshold=1.0)\n",
        "\n",
        "with tf.name_scope(\"dnn\"):\n",
        "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu,\n",
        "                              kernel_regularizer=max_norm_reg, name=\"hidden1\")\n",
        "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu,\n",
        "                              kernel_regularizer=max_norm_reg, name=\"hidden2\")\n",
        "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n",
        "    \n",
        "    \n",
        "    \n",
        "with tf.name_scope(\"loss\"):\n",
        "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
        "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
        "\n",
        "with tf.name_scope(\"train\"):\n",
        "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum)\n",
        "    training_op = optimizer.minimize(loss)    \n",
        "\n",
        "with tf.name_scope(\"eval\"):\n",
        "    correct = tf.nn.in_top_k(logits, y, 1)\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "saver = tf.train.Saver()\n",
        "\n",
        "\n",
        "n_epochs = 20\n",
        "batch_size = 50\n",
        "\n",
        "clip_all_weights = tf.get_collection(\"max_norm\")\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    init.run()\n",
        "    for epoch in range(n_epochs):\n",
        "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
        "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
        "            sess.run(clip_all_weights)\n",
        "        acc_valid = accuracy.eval(feed_dict={X: X_valid, y: y_valid}) \n",
        "        print(epoch, \"Validation accuracy:\", acc_valid)               \n",
        "\n",
        "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 Validation accuracy: 0.9542\n",
            "1 Validation accuracy: 0.968\n",
            "2 Validation accuracy: 0.974\n",
            "3 Validation accuracy: 0.9786\n",
            "4 Validation accuracy: 0.9784\n",
            "5 Validation accuracy: 0.977\n",
            "6 Validation accuracy: 0.9784\n",
            "7 Validation accuracy: 0.9804\n",
            "8 Validation accuracy: 0.9818\n",
            "9 Validation accuracy: 0.9822\n",
            "10 Validation accuracy: 0.9834\n",
            "11 Validation accuracy: 0.9834\n",
            "12 Validation accuracy: 0.9828\n",
            "13 Validation accuracy: 0.9832\n",
            "14 Validation accuracy: 0.9844\n",
            "15 Validation accuracy: 0.9834\n",
            "16 Validation accuracy: 0.9848\n",
            "17 Validation accuracy: 0.9846\n",
            "18 Validation accuracy: 0.9846\n",
            "19 Validation accuracy: 0.9838\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OkN9y9lj881a",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}