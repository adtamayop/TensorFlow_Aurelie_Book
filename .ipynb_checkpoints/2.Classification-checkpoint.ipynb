{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Confusion Matrix**\n",
    "\n",
    "accuracy is generally not the preferred performance measure\n",
    "for classifiers, especially when you are dealing with skewed datasets (i.e., when some\n",
    "classes are much more frequent than others).\n",
    "\n",
    "A much better way to evaluate the performance of a classifier is to look at the confusion\n",
    "matrix. The general idea is to count the number of times instances of class A are\n",
    "classified as class B.\n",
    "\n",
    "You could make predictions on the test set, but let’s keep it untouched for now (remember that you want to use the test set only at the very end of your project, once you have a classifier that you are ready to launch).\n",
    "Instead, you can use the `cross_val_predict() function:`\n",
    "\n",
    "    from sklearn.model_selection import cross_val_predict\n",
    "    y_train_pred = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3)\n",
    "\n",
    "Just like the cross_val_score() function, cross_val_predict() performs K-fold\n",
    "cross-validation, but instead of returning the evaluation scores, it returns the predictions\n",
    "made on each test fold. This means that you get a clean prediction for each\n",
    "instance in the training set (“clean” meaning that the prediction is made by a model\n",
    "that never saw the data during training)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    from sklearn.metrics import confusion_matrix\n",
    "    confusion_matrix(y_train_5, y_train_pred)\n",
    "    array([[53272, 1307],\n",
    "           [ 1077, 4344]])\n",
    "           \n",
    "Each row in a confusion matrix represents an actual class, while each column represents a predicted class.           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confussion matrix          |  formula\n",
    ":-------------------------:|:-------------------------:\n",
    " ![](http://rasbt.github.io/mlxtend/user_guide/evaluate/confusion_matrix_files/confusion_matrix_1.png) |![](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRqVToMYqZ3f-q8NVbjp_Q8W4Y6iNSGihW_s_FxL6ajuM0MUBOV)  \n",
    " \n",
    "**Precisión**\n",
    "La precisión intenta responder a la siguiente pregunta:\n",
    "\n",
    "¿Qué proporción de identificaciones positivas fue correcta?\n",
    "\n",
    "\n",
    "**Exhaustividad (recall)**\n",
    "\n",
    "La exhaustividad intenta responder a la siguiente pregunta:\n",
    "\n",
    "¿Qué proporción de positivos reales se identificó correctamente?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/1.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The ROC Curve**\n",
    "\n",
    "The receiver operating characteristic (ROC) curve is another common tool used with\n",
    "binary classifiers. It is very similar to the precision/recall curve, but instead of plotting\n",
    "precision versus recall, the ROC curve plots the true positive rate (another name\n",
    "for recall) against the false positive rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To plot the ROC curve, you first need to compute the TPR and FPR for various threshold values, using the roc_curve() function:\n",
    "    \n",
    "    from sklearn.metrics import roc_curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_train_5, y_scores)\n",
    "\n",
    "Then you can plot the FPR against the TPR using Matplotlib. This code produces the plot in Figure 3-6:\n",
    "\n",
    "    def plot_roc_curve(fpr, tpr, label=None):\n",
    "    plt.plot(fpr, tpr, linewidth=2, label=label)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.axis([0, 1, 0, 1])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plot_roc_curve(fpr, tpr)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again there is a tradeoff: the higher the recall (TPR), the more false positives\n",
    "(FPR) the classifier produces. The dotted line represents the ROC curve of a purely\n",
    "random classifier; a good classifier stays as far away from that line as possible (toward\n",
    "the top-left corner).\n",
    "\n",
    "One way to compare classifiers is to measure the area under the curve (AUC). A perfect\n",
    "classifier will have a ROC AUC equal to 1, whereas a purely random classifier will\n",
    "have a ROC AUC equal to 0.5. Scikit-Learn provides a function to compute the ROC\n",
    "AUC:\n",
    "\n",
    "        from sklearn.metrics import roc_auc_score\n",
    "        roc_auc_score(y_train_5, y_scores)\n",
    "        0.97061072797174941\n",
    "\n",
    "![](http://danielnee.com/wp-content/uploads/2014/08/ROC2.png)\n",
    "\n",
    "*Since the ROC curve is so similar to the precision/recall (or PR)\n",
    "curve, you may wonder how to decide which one to use. As a rule\n",
    "of thumb, you should prefer the PR curve whenever the positive\n",
    "class is rare or when you care more about the false positives than\n",
    "the false negatives, and the ROC curve otherwise. For example,\n",
    "looking at the previous ROC curve (and the ROC AUC score), you\n",
    "may think that the classifier is really good. But this is mostly\n",
    "because there are few positives (5s) compared to the negatives\n",
    "(non-5s). In contrast, the PR curve makes it clear that the classifier\n",
    "has room for improvement (the curve could be closer to the topright\n",
    "corner).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
