{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Overview Machine Learning Landscape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• **Popular open data repositories:**\n",
    "\n",
    "* [UC Irvine Machine Learning Repository](http://archive.ics.uci.edu/ml/index.php)\n",
    "* [Kaggle datasets](https://www.kaggle.com/datasets)\n",
    "* [Amazon’s AWS datasets](https://registry.opendata.aws/)\n",
    "\n",
    "\n",
    "• **Meta portals (they list open data repositories):**\n",
    "\n",
    "* http://dataportals.org/\n",
    "* http://opendatamonitor.eu/\n",
    "* http://quandl.com/\n",
    "\n",
    "• **Other pages listing many**\n",
    "\n",
    "* [Wikipedia List of datasets for machine learning research](https://en.wikipedia.org/wiki/List_of_datasets_for_machine_learning_research)\n",
    "* [Quora list](https://www.quora.com/Where-can-I-find-large-datasets-open-to-the-public)\n",
    "* [Reddit datsets](https://www.quora.com/Where-can-I-find-large-datasets-open-to-the-public)\n",
    "\n",
    "\n",
    "## Checklist for ML projects\n",
    "\n",
    "1. Frame the problem and look at the big picture.\n",
    "2. Get the data.\n",
    "3. Explore the data to gain insights.\n",
    "4. Prepare the data to better expose the underlying data patterns to Machine Learning\n",
    "algorithms.\n",
    "5. Explore many different models and short-list the best ones.\n",
    "6. Fine-tune your models and combine them into a great solution.\n",
    "7. Present your solution.\n",
    "8. Launch, monitor, and maintain your system.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Frame the Problem**\n",
    "\n",
    "The first question to ask your boss is what exactly is the business objective; building a\n",
    "model is probably not the end goal. How does the company expect to use and benefit\n",
    "from this model? This is important because it will determine how you frame the\n",
    "problem, what algorithms you will select, what performance measure you will use to\n",
    "evaluate your model, and how much effort you should spend tweaking it.\n",
    "\n",
    "The next question to ask is what the current solution looks like (if any). It will often\n",
    "give you a reference performance, as well as insights on how to solve the problem.\n",
    "\n",
    "Okay, with all this information you are now ready to start designing your system.\n",
    "First, you need to frame the problem: is it supervised, unsupervised, or Reinforcement\n",
    "Learning? Is it a classification task, a regression task, or something else? Should you use batch learning or online learning techniques?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Select a Performance Measure**\n",
    "\n",
    "Your next step is to select a performance measure. A typical performance measure for\n",
    "regression problems is the Root Mean Square Error (RMSE). It measures the standard\n",
    "deviation4 of the errors the system makes in its predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check the Assumptions** \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.Get the Data**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Common method for split our data is...\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)\n",
    "    \n",
    "So far we have considered purely random sampling methods. This is generally fine if\n",
    "your dataset is large enough (especially relative to the number of attributes), but if it\n",
    "is not, you run the risk of introducing a significant sampling bias\n",
    "\n",
    "This is called stratified\n",
    "sampling: the population is divided into homogeneous subgroups called strata,\n",
    "and the right number of instances is sampled from each stratum to guarantee that the\n",
    "test set is representative of the overall population.\n",
    "\n",
    "For example, suppose we know that a specific attribute is very important for the purpose of our model, so we must ensure that it is well sampled, an example in scikit learn is:\n",
    "\n",
    "    from sklearn.model_selection import StratifiedShuffleSplit\n",
    "    split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "    for train_index, test_index in split.split(housing, housing[\"income_cat\"]):\n",
    "        strat_train_set = housing.loc[train_index]\n",
    "        strat_test_set = housing.loc[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.Discover and Visualize the Data to Gain Insights**\n",
    "\n",
    "Looking for Correlations\n",
    "Since the dataset is not too large, you can easily compute the standard correlation\n",
    "coefficient (also called Pearson’s r) between every pair of attributes using the corr()\n",
    "method:\n",
    "\n",
    "    corr_matrix = housing.corr()\n",
    "\n",
    "Now let’s look at how much each attribute correlates with the median house value:\n",
    "\n",
    "    corr_matrix[\"median_house_value\"].sort_values(ascending=False)\n",
    "\n",
    "    median_house_value 1.000000\n",
    "    median_income 0.687170\n",
    "    total_rooms 0.135231\n",
    "    housing_median_age 0.114220\n",
    "    households 0.064702\n",
    "    total_bedrooms 0.047865\n",
    "    population -0.026699\n",
    "    longitude -0.047279\n",
    "    latitude -0.142826\n",
    "\n",
    "    \n",
    "The correlation coefficient ranges from –1 to 1. When it is close to 1, it means that\n",
    "there is a strong positive correlation; for example, the median house value tends to go\n",
    "up when the median income goes up. When the coefficient is close to –1, it means\n",
    "that there is a strong negative correlation; you can see a small negative correlation\n",
    "between the latitude and the median house value (i.e., prices have a slight tendency to\n",
    "go down when you go north\n",
    "\n",
    "![alt text](https://upload.wikimedia.org/wikipedia/commons/d/d4/Correlation_examples2.svg\")\n",
    "\n",
    "\n",
    "Another way to check for correlation between attributes is to use Pandas’ scatter_matrix function, which plots every numerical attribute against every other numerical attribute.\n",
    "\n",
    "    from pandas.tools.plotting import scatter_matrix\n",
    "    attributes = [\"median_house_value\", \"median_income\", \"total_rooms\",\"housing_median_age\"]\n",
    "    scatter_matrix(housing[attributes], figsize=(12, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Cleaning**\n",
    "\n",
    "Most Machine Learning algorithms cannot work with missing features, so let’s create\n",
    "a few functions to take care of them. You noticed earlier that the total_bedrooms\n",
    "attribute has some missing values, so let’s fix this. You have three options:\n",
    "\n",
    "* Get rid of the corresponding districts.\n",
    "* Get rid of the whole attribute.\n",
    "* Set the values to some value (zero, the mean, the median, etc.).\n",
    "\n",
    "You can accomplish these easily using DataFrame’s dropna(), drop(), and fillna()\n",
    "methods:\n",
    "\n",
    "**handling Text and Categorical Attributes**\n",
    "\n",
    "Most Machine Learning algorithms prefer\n",
    "to work with numbers anyway, so let’s convert these text labels to numbers.\n",
    "Scikit-Learn provides a transformer for this task called LabelEncoder:\n",
    "\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    encoder = LabelEncoder()\n",
    "    housing_cat = housing[\"ocean_proximity\"]\n",
    "    housing_cat_encoded = encoder.fit_transform(housing_cat)\n",
    "    housing_cat_encoded\n",
    "    array([1, 1, 4, ..., 1, 0, 3])\n",
    "    \n",
    "One issue with this representation is that ML algorithms will assume that two nearby\n",
    "values are more similar than two distant values. Obviously this is not the case (for\n",
    "example, categories 0 and 4 are more similar than categories 0 and 1). To fix this\n",
    "issue, a common solution is to create one binary attribute per category: one attribute\n",
    "equal to 1 when the category is “<1H OCEAN” (and 0 otherwise), another attribute\n",
    "equal to 1 when the category is “INLAND” (and 0 otherwise), and so on. This is\n",
    "called one-hot encoding, because only one attribute will be equal to 1 (hot), while the\n",
    "others will be 0 (cold).\n",
    "\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    encoder = OneHotEncoder()\n",
    "    housing_cat_1hot = encoder.fit_transform(housing_cat_encoded)\n",
    "    housing_cat_1hot.toarray()\n",
    "    \n",
    "    array([[ 0., 1., 0., 0., 0.],\n",
    "    [ 0., 1., 0., 0., 0.],\n",
    "    [ 0., 0., 0., 0., 1.],\n",
    "    ...,\n",
    "    [ 0., 1., 0., 0., 0.],"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply both transformations (from text categories to integer categories, then\n",
    "from integer categories to one-hot vectors) in one shot using the LabelBinarizer\n",
    "class:\n",
    "\n",
    "    >>> from sklearn.preprocessing import LabelBinarizer\n",
    "    >>> encoder = LabelBinarizer()\n",
    "    >>> housing_cat_1hot = encoder.fit_transform(housing_cat)\n",
    "    >>> housing_cat_1hot\n",
    "    array([[0, 1, 0, 0, 0],\n",
    "    [0, 1, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 1],\n",
    "    ...,\n",
    "    [0, 1, 0, 0, 0],\n",
    "    [1, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 1, 0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature Scaling**\n",
    "\n",
    "One of the most important transformations you need to apply to your data is feature\n",
    "scaling. With few exceptions, Machine Learning algorithms don’t perform well when\n",
    "the input numerical attributes have very different scales.\n",
    "\n",
    "There are two common ways to get all attributes to have the same scale: min-max\n",
    "scaling and standardization.\n",
    "Min-max scaling (many people call this normalization) is quite simple: values are\n",
    "shifted and rescaled so that they end up ranging from 0 to 1. We do this by subtracting\n",
    "the min value and dividing by the max minus the min. Scikit-Learn provides a\n",
    "transformer called MinMaxScaler for this. It has a feature_range hyperparameter\n",
    "that lets you change the range if you don’t want 0–1 for some reason.\n",
    "\n",
    "Standardization is quite different: first it subtracts the mean value (so standardized\n",
    "values always have a zero mean), and then it divides by the variance so that the resulting\n",
    "distribution has unit variance. Unlike min-max scaling, standardization does not\n",
    "bound values to a specific range, which may be a problem for some algorithms (e.g.,\n",
    "neural networks often expect an input value ranging from 0 to 1). However, standardization\n",
    "is much less affected by outliers. For example, suppose a district had a median\n",
    "income equal to 100 (by mistake). Min-max scaling would then crush all the other\n",
    "values from 0–15 down to 0–0.15, whereas standardization would not be much affected.\n",
    "Scikit-Learn provides a transformer called StandardScaler for standardization.\n",
    "\n",
    "*As with all the transformations, it is important to fit the scalers to\n",
    "the training data only, not to the full dataset (including the test set).\n",
    "Only then can you use them to transform the training set and the\n",
    "test set (and new data).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transformation Pipelines**\n",
    "\n",
    "As you can see, there are many data transformation steps that need to be executed in\n",
    "the right order\n",
    "\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    num_pipeline = Pipeline([\n",
    "                            ('imputer', Imputer(strategy=\"median\")),\n",
    "                            ('attribs_adder', CombinedAttributesAdder()),\n",
    "                            ('std_scaler', StandardScaler()),\n",
    "                            ])\n",
    "    housing_num_tr = num_pipeline.fit_transform(housing_num)\n",
    "    \n",
    "    \n",
    "**Training and Evaluating on the Training Set**\n",
    "\n",
    "**Better Evaluation Using Cross-Validation**\n",
    "\n",
    "One way to evaluate the Decision Tree model would be to use the train_test_split\n",
    "function to split the training set into a smaller training set and a validation set, then\n",
    "train your models against the smaller training set and evaluate them against the validation\n",
    "set. It’s a bit of work, but nothing too difficult and it would work fairly well.\n",
    "\n",
    "A great alternative is to use Scikit-Learn’s cross-validation feature. The following code\n",
    "performs K-fold cross-validation: it randomly splits the training set into 10 distinct\n",
    "subsets called folds, then it trains and evaluates the Decision Tree model 10 times,\n",
    "picking a different fold for evaluation every time and training on the other 9 folds.\n",
    "The result is an array containing the 10 evaluation scores:\n",
    "\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    scores = cross_val_score(tree_reg, housing_prepared, housing_labels,\n",
    "    scoring=\"neg_mean_squared_error\", cv=10)\n",
    "    rmse_scores = np.sqrt(-scores)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-Learn cross-validation features expect a utility function\n",
    "(greater is better) rather than a cost function (lower is better), so\n",
    "the scoring function is actually the opposite of the MSE (i.e., a negative\n",
    "value), which is why the preceding code computes -scores\n",
    "before calculating the square root."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For save a model simply: \n",
    "\n",
    "    from sklearn.externals import joblib\n",
    "    joblib.dump(my_model, \"my_model.pkl\")\n",
    "    # and later...\n",
    "    my_model_loaded = joblib.load(\"my_model.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fine-Tune Your Model**\n",
    "\n",
    "**GridSearch**\n",
    "\n",
    "you should get Scikit-Learn’s GridSearchCV to search for you. All you need to\n",
    "do is tell it which hyperparameters you want it to experiment with, and what values to\n",
    "try out, and it will evaluate all the possible combinations of hyperparameter values,\n",
    "using cross-validation.\n",
    "\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    param_grid = [\n",
    "    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n",
    "    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},]\n",
    "                                        \n",
    "    forest_reg = RandomForestRegressor()\n",
    "    grid_search = GridSearchCV(forest_reg, param_grid, cv=5,scoring='neg_mean_squared_error')\n",
    "    grid_search.fit(housing_prepared, housing_labels)\n",
    "    \n",
    "**Randomized Search**\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
