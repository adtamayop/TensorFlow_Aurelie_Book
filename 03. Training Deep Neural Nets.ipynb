{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Deep Neural Nets\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Content \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanishing/Exploding Gradients Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The backpropagation algorithm works by going from the output layer to the input layer, propagating the error gradient on the way. Once the algorithm has computed the gradient of the cost function with regards to each parameter in the network, it uses these gradients to update each parameter with a Gradient Descent step.\n",
    "\n",
    "Unfortunately, gradients often get *smaller and smaller* as the algorithm progresses down to the lower layers. As a result, the Gradient Descent update leaves the lower layer connection weights virtually unchanged, and training never converges to a good solution. This is called the `vanishing gradients problem.`\n",
    "\n",
    "In some cases, the opposite can happen: the gradients can grow bigger and bigger, so many layers get insanely \n",
    "large weight updates and the algorithm diverges. This is the exploding gradients problem, \n",
    "which is mostly encountered in recurrent neural networks\n",
    "\n",
    "Looking at the logistic activation function , you can see that when\n",
    "inputs become large (negative or positive), the function saturates at 0 or 1, with a\n",
    "derivative extremely close to 0. Thus when backpropagation kicks in, it has virtually\n",
    "no gradient to propagate back through the network, and what little gradient exists\n",
    "keeps getting diluted as backpropagation progresses down through the top layers, so\n",
    "there is really nothing left for the lower layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "def logit(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "z = np.linspace(-5, 5, 200)\n",
    "\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([-5, 5], [1, 1], 'k--')\n",
    "plt.plot([0, 0], [-0.2, 1.2], 'k-')\n",
    "plt.plot([-5, 5], [-3/4, 7/4], 'g--')\n",
    "plt.plot(z, logit(z), \"b-\", linewidth=2)\n",
    "props = dict(facecolor='black', shrink=0.1)\n",
    "plt.annotate('Saturating', xytext=(3.5, 0.7), xy=(5, 1), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.annotate('Saturating', xytext=(-3.5, 0.3), xy=(-5, 0), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.annotate('Linear', xytext=(2, 0.2), xy=(0, 0.5), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.grid(True)\n",
    "plt.title(\"Sigmoid activation function\", fontsize=14)\n",
    "plt.axis([-5, 5, -0.2, 1.2])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To resolve this problem the authors argue that we need the variance of the\n",
    "outputs of each layer to be equal to the variance of its inputs, and we also need the\n",
    "gradients to have equal variance before and after flowing through a layer in the\n",
    "reverse direction\n",
    "\n",
    "It is actually not possible to guarantee both unless the layer has an equal\n",
    "number of input and output connections, but they proposed a good compromise that\n",
    "has proven to work very well in practice: the connection weights must be initialized\n",
    "randomly\n",
    "\n",
    "This initialization strategy is often called Xavier initialization\n",
    "(after the author’s first name), or sometimes Glorot initialization.\n",
    "\n",
    "Using the Xavier initialization strategy can speed up training considerably, and it is\n",
    "one of the tricks that led to the current success of Deep Learning.\n",
    "\n",
    "By default, the fully_connected() function (introduced in Chapter 10) uses Xavier\n",
    "initialization (with a uniform distribution). You can change this to He initialization\n",
    "by using the variance_scaling_initializer() function like this:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "\n",
    "he_init = tf.variance_scaling_initializer()\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu,\n",
    "                          kernel_initializer=he_init, name=\"hidden1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nonsaturating Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReLU is better than logistic function, unfortunately, the ReLU activation function is not perfect. It suffers from a problem\n",
    "known as the dying ReLUs: during training, some neurons effectively die, meaning\n",
    "they stop outputting anything other than 0. In some cases, you may find that half of\n",
    "your network’s neurons are dead, especially if you used a large learning rate. During\n",
    "training, if a neuron’s weights get updated such that the weighted sum of the neuron’s\n",
    "inputs is negative, it will start outputting 0. When this happen, the neuron is unlikely\n",
    "to come back to life since the gradient of the ReLU function is 0 when its input is\n",
    "negative.\n",
    "\n",
    "To solve this problem, you may want to use a variant of the ReLU function, such as\n",
    "the leaky ReLU. This function is defined as LeakyReLUα(z) = max(αz, z)\n",
    "\n",
    "The hyperparameter α defines how much the function “leaks”: it is the\n",
    "slope of the function for z < 0, and is typically set to 0.01. This small slope ensures\n",
    "that leaky ReLUs never die; they can go into a long coma, but they have a chance to\n",
    "eventually wake up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### leaky ReLUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def leaky_relu(z, alpha=0.01):\n",
    "    return np.maximum(alpha*z, z)\n",
    "\n",
    "plt.plot(z, leaky_relu(z, 0.05), \"b-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([0, 0], [-0.5, 4.2], 'k-')\n",
    "plt.grid(True)\n",
    "props = dict(facecolor='black', shrink=0.1)\n",
    "plt.annotate('Leak', xytext=(-3.5, 0.5), xy=(-5, -0.2), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.title(\"Leaky ReLU activation function\", fontsize=14)\n",
    "plt.axis([-5, 5, -0.5, 4.2])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implementing Leakly Relu in TF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Batch accuracy: 0.88 Validation accuracy: 0.9046\n",
      "5 Batch accuracy: 0.92 Validation accuracy: 0.9496\n",
      "10 Batch accuracy: 0.94 Validation accuracy: 0.9644\n",
      "15 Batch accuracy: 0.98 Validation accuracy: 0.9708\n",
      "20 Batch accuracy: 0.98 Validation accuracy: 0.974\n",
      "25 Batch accuracy: 1.0 Validation accuracy: 0.9756\n",
      "30 Batch accuracy: 1.0 Validation accuracy: 0.9772\n",
      "35 Batch accuracy: 1.0 Validation accuracy: 0.9798\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "\n",
    "def leaky_relu(z, name=None):\n",
    "    return tf.maximum(0.01 * z, z, name=name)\n",
    "\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, activation=leaky_relu, name=\"hidden1\")\n",
    "\n",
    "# Let's train a neural network on MNIST using the Leaky ReLU. First let's create the graph:\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=leaky_relu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=leaky_relu, name=\"hidden2\")\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "X_train = X_train.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "X_test = X_test.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "y_train = y_train.astype(np.int32)\n",
    "y_test = y_test.astype(np.int32)\n",
    "X_valid, X_train = X_train[:5000], X_train[5000:]\n",
    "y_valid, y_train = y_train[:5000], y_train[5000:]\n",
    "\n",
    "def shuffle_batch(X, y, batch_size):\n",
    "    rnd_idx = np.random.permutation(len(X))\n",
    "    n_batches = len(X) // batch_size\n",
    "    for batch_idx in np.array_split(rnd_idx, n_batches):\n",
    "        X_batch, y_batch = X[batch_idx], y[batch_idx]\n",
    "        yield X_batch, y_batch\n",
    "\n",
    "n_epochs = 40\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        if epoch % 5 == 0:\n",
    "            acc_batch = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "            acc_valid = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "            print(epoch, \"Batch accuracy:\", acc_batch, \"Validation accuracy:\", acc_valid)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ELU activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing ELU in TensorFlow is trivial, just specify the activation function when building each layer:\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.elu, name=\"hidden1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def elu(z, alpha=1):\n",
    "    return np.where(z < 0, alpha * (np.exp(z) - 1), z)\n",
    "\n",
    "plt.plot(z, elu(z), \"b-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([-5, 5], [-1, -1], 'k--')\n",
    "plt.plot([0, 0], [-2.2, 3.2], 'k-')\n",
    "plt.grid(True)\n",
    "plt.title(r\"ELU activation function ($\\alpha=1$)\", fontsize=14)\n",
    "plt.axis([-5, 5, -2.2, 3.2])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main drawback of the ELU activation function is that it is slower to compute\n",
    "than the ReLU and its variants (due to the use of the exponential function), but during\n",
    "training this is compensated by the faster convergence rate. However, at test time\n",
    "an ELU network will be slower than a ReLU network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** Which choose?!:\n",
    "    \n",
    "So which activation function should you use for the hidden layers\n",
    "of your deep neural networks? Although your mileage will vary, in\n",
    "general ELU > leaky ReLU (and its variants) > ReLU > tanh > logistic.\n",
    "If you care a lot about runtime performance, then you may prefer\n",
    "leaky ReLUs over ELUs. If you don’t want to tweak yet another\n",
    "hyperparameter, you may just use the default α values suggested\n",
    "earlier (0.01 for the leaky ReLU, and 1 for ELU). If you have spare\n",
    "time and computing power, you can use cross-validation to evaluate\n",
    "other activation functions, in particular RReLU if your network\n",
    "is overfitting, or PReLU if you have a huge training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Normalization\n",
    "\n",
    "Although using He initialization along with ELU (or any variant of ReLU) can significantly\n",
    "reduce the vanishing/exploding gradients problems at the beginning of training,\n",
    "it doesn’t guarantee that they won’t come back during training.\n",
    "\n",
    "The technique consists of adding an operation in the model just before the activation\n",
    "function of each layer, simply zero-centering and normalizing the inputs, then scaling\n",
    "and shifting the result using two new parameters per layer (one for scaling, the other\n",
    "for shifting). **In other words, this operation lets the model learn the optimal scale and\n",
    "mean of the inputs for each layer.**\n",
    "\n",
    "In order to zero-center and normalize the inputs, the algorithm needs to estimate the\n",
    "inputs’ mean and standard deviation. It does so by evaluating the mean and standard\n",
    "deviation of the inputs over the current mini-batch (hence the name “Batch Normalization\n",
    "\n",
    "The authors demonstrated that this technique considerably improved all the deep\n",
    "neural networks they experimented with. The vanishing gradients problem was\n",
    "strongly reduced, to the point that they could use saturating activation functions such\n",
    "as the tanh and even the logistic activation function. The networks were also much\n",
    "less sensitive to the weight initialization. They were able to use much larger learning\n",
    "rates, significantly speeding up the learning process\n",
    "\n",
    "Batch Normalization also acts like a regularizer, reducing the need for other regularization techniques (Dropout)\n",
    "\n",
    "Batch Normalization does, however, add some complexity to the model (although it \n",
    "removes the need for normalizing the input data since the first hidden layer will take\n",
    "care of that, provided it is batch-normalized). Moreover, there is a runtime penalty:\n",
    "the neural network makes slower predictions due to the extra computations required\n",
    "at each layer. So if you need predictions to be lightning-fast, you may want to check\n",
    "how well plain ELU + He initialization perform before playing with Batch Normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "\n",
    "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\")\n",
    "bn1 = tf.layers.batch_normalization(hidden1, training=training, momentum=0.9)\n",
    "bn1_act = tf.nn.elu(bn1)\n",
    "\n",
    "hidden2 = tf.layers.dense(bn1_act, n_hidden2, name=\"hidden2\")\n",
    "bn2 = tf.layers.batch_normalization(hidden2, training=training, momentum=0.9)\n",
    "bn2_act = tf.nn.elu(bn2)\n",
    "\n",
    "logits_before_bn = tf.layers.dense(bn2_act, n_outputs, name=\"outputs\")\n",
    "logits = tf.layers.batch_normalization(logits_before_bn, training=training,\n",
    "                                       momentum=0.9)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "training = tf.placeholder_with_default(False, shape=(), name='training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To avoid repeating the same parameters over and over again, we can use Python's partial() function:\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "my_batch_norm_layer = partial(tf.layers.batch_normalization,\n",
    "                              training=training, momentum=0.9)\n",
    "\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\")\n",
    "bn1 = my_batch_norm_layer(hidden1)\n",
    "bn1_act = tf.nn.elu(bn1)\n",
    "hidden2 = tf.layers.dense(bn1_act, n_hidden2, name=\"hidden2\")\n",
    "bn2 = my_batch_norm_layer(hidden2)\n",
    "bn2_act = tf.nn.elu(bn2)\n",
    "logits_before_bn = tf.layers.dense(bn2_act, n_outputs, name=\"outputs\")\n",
    "logits = my_batch_norm_layer(logits_before_bn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Practical example with MNIST and Normalization\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "batch_norm_momentum = 0.9\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    he_init = tf.variance_scaling_initializer()\n",
    "\n",
    "    my_batch_norm_layer = partial(\n",
    "            tf.layers.batch_normalization,\n",
    "            training=training,\n",
    "            momentum=batch_norm_momentum)\n",
    "\n",
    "    my_dense_layer = partial(\n",
    "            tf.layers.dense,\n",
    "            kernel_initializer=he_init)\n",
    "\n",
    "    hidden1 = my_dense_layer(X, n_hidden1, name=\"hidden1\")\n",
    "    bn1 = tf.nn.elu(my_batch_norm_layer(hidden1))\n",
    "    hidden2 = my_dense_layer(bn1, n_hidden2, name=\"hidden2\")\n",
    "    bn2 = tf.nn.elu(my_batch_norm_layer(hidden2))\n",
    "    logits_before_bn = my_dense_layer(bn2, n_outputs, name=\"outputs\")\n",
    "    logits = my_batch_norm_layer(logits_before_bn)\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Validation accuracy: 0.894\n",
      "1 Validation accuracy: 0.9138\n",
      "2 Validation accuracy: 0.9286\n",
      "3 Validation accuracy: 0.94\n",
      "4 Validation accuracy: 0.9472\n",
      "5 Validation accuracy: 0.9522\n",
      "6 Validation accuracy: 0.9544\n",
      "7 Validation accuracy: 0.957\n",
      "8 Validation accuracy: 0.9606\n",
      "9 Validation accuracy: 0.9634\n",
      "10 Validation accuracy: 0.964\n",
      "11 Validation accuracy: 0.965\n",
      "12 Validation accuracy: 0.9668\n",
      "13 Validation accuracy: 0.9684\n",
      "14 Validation accuracy: 0.9692\n",
      "15 Validation accuracy: 0.9712\n",
      "16 Validation accuracy: 0.9712\n",
      "17 Validation accuracy: 0.9722\n",
      "18 Validation accuracy: 0.973\n",
      "19 Validation accuracy: 0.9738\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 200\n",
    "\n",
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run([training_op, extra_update_ops],\n",
    "                     feed_dict={training: True, X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"Validation accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Clipping\n",
    "\n",
    "A popular technique to lessen the exploding gradients problem is to simply clip the\n",
    "gradients during backpropagation so that they never exceed some threshold (this is mostly useful for recurrent neural networks). This is called Gradient Clipping\n",
    "\n",
    "In general people now prefer Batch Normalization, but it’s still useful to know about Gradient Clipping and how to implement it.\n",
    "In TensorFlow, the optimizer’s minimize() function takes care of both computing the gradients and applying them, so you must instead call the optimizer’s `compute_gradients()` method first, then create an operation to clip the gradients using the `clip_by_value()` function, and finally create an operation to apply the clipped gradients using the optimizer’s `apply_gradients()` method:\n",
    "\n",
    "    threshold = 1.0\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    grads_and_vars = optimizer.compute_gradients(loss)\n",
    "    capped_gvs = [(tf.clip_by_value(grad, -threshold, threshold), var) for grad, var in grads_and_vars]\n",
    "    training_op = optimizer.apply_gradients(capped_gvs)\n",
    "\n",
    "You would then run this training_op at every training step, as usual. It will compute\n",
    "the gradients, clip them between –1.0 and 1.0, and apply them. The threshold is a\n",
    "hyperparameter you can tune."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Faster Optimizers\n",
    "\n",
    "So far we have seen four ways to speed up training (and reach a better solution): \n",
    "* applying a good initialization strategy for the connection weights\n",
    "* using a good activation function\n",
    "* using Batch Normalization\n",
    "* reusing parts of a pretrained network. \n",
    "\n",
    "Another huge speed boost comes from using a faster optimizer than the regular Gradient Descent optimizer, the most popular ones: \n",
    "- Momentum optimization\n",
    "- Nesterov Accelerated Gradient\n",
    "- AdaGrad\n",
    "- RMSProp\n",
    "- **Adam optimization.**\n",
    "\n",
    "### **you should almost always use Adam optimization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Momentum optimization\n",
    "\n",
    "Momentum optimization cares a great deal about what previous gradients were: at\n",
    "each iteration, it adds the local gradient to the momentum vector m (multiplied by the\n",
    "learning rate η), and it updates the weights by simply subtracting this momentum\n",
    "vector \n",
    "\n",
    "In other words, the gradient is used as an acceleration, not\n",
    "as a speed. To simulate some sort of friction mechanism and prevent the momentum\n",
    "from growing too large, the algorithm introduces a new hyperparameter β, simply\n",
    "called the momentum, which must be set between 0 (high friction) and 1 (no friction).\n",
    "A typical momentum value is 0.9.\n",
    "\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate,momentum=0.9)\n",
    "\n",
    "The one drawback of Momentum optimization is that it adds yet another hyperparameter\n",
    "to tune. However, the momentum value of 0.9 usually works well in practice\n",
    "and almost always goes faster than Gradient Descent.\n",
    "\n",
    "\n",
    "# Nesterov Accelerated Gradient\n",
    "\n",
    "is almost always faster than vanilla Momentum optimization. The idea of Nesterov\n",
    "Momentum optimization, or Nesterov Accelerated Gradient (NAG), is to measure the\n",
    "gradient of the cost function not at the local position but slightly ahead in the direction\n",
    "of the momentum\n",
    "\n",
    "NAG will almost always speed up training compared to regular Momentum optimization.\n",
    "To use it, simply set use_nesterov=True when creating the MomentumOptim\n",
    "izer:\n",
    "\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate,momentum=0.9, use_nesterov=True)\n",
    "    \n",
    "# AdaGrad\n",
    "\n",
    "Consider the elongated bowl problem again: Gradient Descent starts by quickly going\n",
    "down the steepest slope, then slowly goes down the bottom of the valley. It would be\n",
    "nice if the algorithm could detect this early on and correct its direction to point a bit\n",
    "more toward the global optimum.    \n",
    "\n",
    "The AdaGrad algorithm13 achieves this by scaling down the gradient vector along the\n",
    "steepest dimensions\n",
    "\n",
    "**In short, this algorithm decays the learning rate, but it does so faster for steep dimensions\n",
    "than for dimensions with gentler slopes. This is called an adaptive learning rate.\n",
    "It helps point the resulting updates more directly toward the global optimum One additional benefit is that it requires much less tuning of the\n",
    "learning rate hyperparameter η.**\n",
    "\n",
    "    optimizer = tf.train.AdagradOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "# RMSProp\n",
    "\n",
    "Although AdaGrad slows down a bit too fast and ends up never converging to the\n",
    "global optimum, the RMSProp algorithm14 fixes this by accumulating only the gradients\n",
    "from the most recent iterations\n",
    "\n",
    "        optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate,momentum=0.9, decay=0.9, epsilon=1e-10)\n",
    "        \n",
    "Except on very simple problems, this optimizer almost always performs much better\n",
    "than AdaGrad. It also generally performs better than Momentum optimization and\n",
    "Nesterov Accelerated Gradients. In fact, it was the preferred optimization algorithm\n",
    "of many researchers until Adam optimization came around.\n",
    "\n",
    "\n",
    "# Adam Optimization\n",
    "\n",
    "Adam,15 which stands for adaptive moment estimation, combines the ideas of Momentum\n",
    "optimization and RMSProp: just like Momentum optimization it keeps track of\n",
    "an exponentially decaying average of past gradients, and just like RMSProp it keeps\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate) \n",
    "\n",
    "In fact, since Adam is an adaptive learning rate algorithm (like AdaGrad and\n",
    "RMSProp), it requires less tuning of the learning rate hyperparameter η. You can\n",
    "often use the default value η = 0.001, making Adam even easier to use than Gradient\n",
    "Descent.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training Sparse Models** \n",
    "\n",
    "All the optimization algorithms just presented produce dense models, meaning that\n",
    "most parameters will be nonzero. If you need a blazingly fast model at runtime, or if\n",
    "you need it to take up less memory, you may prefer to end up with a sparse model\n",
    "instead.\n",
    "One trivial way to achieve this is to train the model as usual, then get rid of the tiny\n",
    "weights (set them to 0).\n",
    "Another option is to apply strong ℓ1 regularization during training, as it pushes the\n",
    "optimizer to zero out as many weights as it can \n",
    "\n",
    "However, in some cases these techniques may remain insufficient. One last option is\n",
    "to apply Dual Averaging, often called Follow The Regularized Leader (FTRL), a technique\n",
    "proposed by Yurii Nesterov. When used with ℓ1 regularization, this technique\n",
    "often leads to very sparse models. TensorFlow implements a variant of FTRL called\n",
    "FTRL-Proximal18 in the FTRLOptimizer class."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Rate Scheduling\n",
    "\n",
    "Finding a good learning rate can be tricky. If you set it way too high, training may\n",
    "actually diverge  If you set it too low, training will\n",
    "eventually converge to the optimum, but it will take a very long time. \n",
    "\n",
    "If you set it slightly too high, it will make progress very quickly at first, but it will end up dancing\n",
    "around the optimum, never settling down (unless you use an adaptive learning rate optimization algorithm such as AdaGrad, RMSProp, or Adam, but even then it may take time to settle). If you have a limited computing budget, you may have to interrupt training before it has converged properly, yielding a suboptimal solution\n",
    "\n",
    "<img src=\"https://www.researchgate.net/profile/Kazuyuki_Hara/publication/274340363/figure/fig2/AS:667598840659969@1536179375085/Generalization-error-of-the-perceptron-using-true-gradient-for-different-learning-rates.png\">\n",
    "\n",
    "You may be able to find a fairly good learning rate by training your network several times during just a few epochs using various learning rates and comparing the learning curves. \n",
    "\n",
    "The ideal learning rate will learn quickly and converge to good solution.\n",
    "\n",
    "However, you can do better than a constant learning rate: if you start with a high\n",
    "learning rate and then reduce it once it stops making fast progress, you can reach a\n",
    "good solution faster than with the optimal constant learning rate. \n",
    "\n",
    "There are many different strategies to reduce the learning rate during training. These strategies are called\n",
    "learning schedules the most common of which are:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predetermined piecewise constant learning rate:** For example, set the learning rate to η0 = 0.1 at first, then to η1 = 0.001 after 50 epochs. Although this solution can work very well, it often requires fiddling around to figure out the right learning rates and when to use them.\n",
    "\n",
    "**Performance scheduling:** Measure the validation error every N steps (just like for early stopping) and\n",
    "reduce the learning rate by a factor of λ when the error stops dropping.\n",
    "\n",
    "**Exponential scheduling:** Set the learning rate to a function of the iteration number t: η(t) = η0 10–t/r. This\n",
    "works great, but it requires tuning η0 and r. The learning rate will drop by a factor of 10 every r steps.\n",
    "\n",
    "**Power scheduling:** Set the learning rate to η(t) = η0 (1 + t/r)–c. The hyperparameter c is typically set\n",
    "to 1. This is similar to exponential scheduling, but the learning rate drops much\n",
    "more slowly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The authors concluded that, in this setting, both performance scheduling and exponential scheduling performed well, but they favored exponential scheduling because it is simpler to implement, is easy to tune,and converged slightly faster to the optimal solution.\n",
    "\n",
    "*Implementing a learning schedule with TensorFlow is fairly straightforward:*\n",
    "\n",
    "    initial_learning_rate = 0.1\n",
    "    decay_steps = 10000\n",
    "    decay_rate = 1/10\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    learning_rate = tf.train.exponential_decay(initial_learning_rate, global_step, decay_steps, decay_rate)\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=0.9)\n",
    "    training_op = optimizer.minimize(loss, global_step=global_step)\n",
    "    \n",
    "Since AdaGrad, RMSProp, and Adam optimization automatically reduce the learning rate during training, it is not necessary to add an extra learning schedule. For other optimization algorithms, using exponential decay or performance scheduling can\n",
    "considerably speed up convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avoiding Overfitting Through Regularization\n",
    "\n",
    "### Early Stopping\n",
    "\n",
    "To avoid overfitting the training set, a great solution is early stopping training when its performance on the validation set starts dropping.\n",
    "\n",
    "One way to implement this with TensorFlow is to evaluate the model on a validation set at regular intervals (e.g., every 50 steps), and save a “winner” snapshot if it outperforms previous “winner” snapshots. Count the number of steps since the last “winner” snapshot was saved, and interrupt training when this number reaches some limit\n",
    "(e.g., 2,000 steps). Then restore the last “winner” snapshot.\n",
    "\n",
    "Although early stopping works very well in practice, you can usually get much higher\n",
    "performance out of your network by combining it with other regularization techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ℓ1 and ℓ2 Regularization\n",
    "\n",
    "you can use ℓ1 and ℓ2 regularization to constrain a neural network’s connection weights\n",
    "\n",
    "One way to do this using TensorFlow is to simply add the appropriate regularization terms to your cost function. For example, assuming you have just one hidden layer with weights weights1 and one output layer with weights weights2, then you can apply ℓ1 regularization like this:\n",
    "\n",
    "    base_loss = tf.reduce_mean(xentropy, name=\"avg_xentropy\")\n",
    "    reg_losses = tf.reduce_sum(tf.abs(weights1)) + tf.reduce_sum(tf.abs(weights2))\n",
    "    loss = tf.add(base_loss, scale * reg_losses, name=\"loss\")\n",
    "    \n",
    "However, if there are many layers, this approach is not very convenient. Fortunately,TensorFlow provides a better option.\n",
    "\n",
    "Many functions that create variables (such as `get_variable()` or `fully_connected()`) accept a `*_regularizer` argument for each created variable (e.g., weights_regularizer). You can pass any function that takes weights as an argument and returns the corresponding regularization loss. The `l1_regularizer()`, `l2_regularizer()`, and `l1_l2_regularizer()` functions return such functions. The following code puts all this together:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ymYJPpgswxl3"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "#\n",
    "# Implementation of regularization \n",
    "#\n",
    "scale = 0.001\n",
    "\n",
    "my_dense_layer = partial(\n",
    "    tf.layers.dense, activation=tf.nn.relu,\n",
    "    kernel_regularizer=tf.contrib.layers.l1_regularizer(scale))\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = my_dense_layer(X, n_hidden1, name=\"hidden1\")\n",
    "    hidden2 = my_dense_layer(hidden1, n_hidden2, name=\"hidden2\")\n",
    "    logits = my_dense_layer(hidden2, n_outputs, activation=None,name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):                                   \n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)                                \n",
    "    base_loss = tf.reduce_mean(xentropy, name=\"avg_xentropy\")   \n",
    "    reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    loss = tf.add_n([base_loss] + reg_losses, name=\"loss\")    \n",
    "    \n",
    "\n",
    "#\n",
    "# Rest is normal \n",
    "#\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5SQ_pheZwxpP"
   },
   "outputs": [],
   "source": [
    "\n",
    "n_epochsn_epochs = 20\n",
    "batch_size = 200\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"Validation accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a5DTAlbDo_b4"
   },
   "source": [
    "This code creates a neural network with two hidden layers and one output layer, and\n",
    "it also creates nodes in the graph to compute the ℓ1 regularization loss corresponding\n",
    "to each layer’s weights. TensorFlow automatically adds these nodes to a special collection\n",
    "containing all the regularization losses. You just need to add these regularization\n",
    "losses to your overall loss, like this:\n",
    "            \n",
    "            reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "            loss = tf.add_n([base_loss] + reg_losses, name=\"loss\")\n",
    "            \n",
    "  \n",
    "  \n",
    " # Dropout \n",
    " \n",
    " It is a fairly simple algorithm: at every training step, every neuron (including the\n",
    "input neurons but excluding the output neurons) has a **probability p** of being temporarily\n",
    "“dropped out,” meaning it will be entirely ignored during this training step,\n",
    "but it may be active during the next step. The hyperparameter p is\n",
    "called the dropout rate, and it is typically set to 50%. After training, neurons don’t get\n",
    "dropped anymore.\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/644/1*dEi_IkVB7IpkzZ-6H0Vpsg.png\">\n",
    "\n",
    "\n",
    "Neurons trained with dropout cannot co-adapt with their neighboring neurons; they have to be as useful as possible on their own. They also cannot rely excessively on just a few input neurons; they must pay attention to each of their input neurons. They end up being less sensitive to slight\n",
    "changes in the inputs. In the end you get a more robust network that generalizes better.\n",
    "\n",
    "There is one small but important technical detail. Suppose p = 50, in which case during\n",
    "testing a neuron will be connected to twice as many input neurons as it was (on\n",
    "average) during training. To compensate for this fact, we need to multiply each neuron\n",
    "input connection weights by 0.5 after training. If we don’t, each neuron will get a\n",
    "total input signal roughly twice as large as what the network was trained on, and it is\n",
    "unlikely to perform well. More generally, we need to multiply each input connection\n",
    "weight by the keep probability (1 – p) after training. Alternatively, we can divide each\n",
    "neuron’s output by the keep probability during training (these alternatives are not\n",
    "perfectly equivalent, but they work equally well).\n",
    "\n",
    "An example of the implementation of the dropout regularization is the next code:\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nfnjbmD9pVBd"
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "\n",
    "dropout_rate = 0.5  # == 1 - keep_prob\n",
    "X_drop = tf.layers.dropout(X, dropout_rate, training=training)\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X_drop, n_hidden1, activation=tf.nn.relu)\n",
    "    hidden1_drop = tf.layers.dropout(hidden1, dropout_rate, training=training)\n",
    "    hidden2 = tf.layers.dense(hidden1_drop, n_hidden2, activation=tf.nn.relu)\n",
    "    hidden2_drop = tf.layers.dropout(hidden2, dropout_rate, training=training)\n",
    "    logits = tf.layers.dense(hidden2_drop, n_outputs)\n",
    "    \n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=0.9)\n",
    "    training_op = optimizer.minimize(loss)    \n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "\n",
    "n_epochs = 20\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch, training: True})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"Validation accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nA4qFaIV5xLl"
   },
   "source": [
    "     \n",
    "*You want to use the dropout() function in tensorflow.con\n",
    "trib.layers, not the one in tensorflow.nn. The first one turns off\n",
    "(no-op) when not training, which is what you want, while the second\n",
    "one does not.*\n",
    "\n",
    "Of course, just like you did earlier for Batch Normalization, you need to set is_train\n",
    "ing to True when training, and to False when testing.\n",
    "\n",
    "If you observe that the model is overfitting, you can increase the dropout rate (i.e.,\n",
    "reduce the keep_prob hyperparameter). Conversely, you should try decreasing the\n",
    "dropout rate (i.e., increasing keep_prob) if the model underfits the training set. It can\n",
    "also help to increase the dropout rate for large layers, and reduce it for small ones.\n",
    "\n",
    "Dropout does tend to significantly slow down convergence, but it usually results in a\n",
    "much better model when tuned properly. So, it is generally well worth the extra time\n",
    "and effort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rDcAyPAE6ySC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rMEJRDo07eIp"
   },
   "source": [
    "# Max-Norm Regularization\n",
    "\n",
    "Another regularization technique that is quite popular for neural networks is called\n",
    "max-norm regularization: for each neuron, it constrains the weights **w** of the incoming\n",
    "connections such that ∥ w ∥2 ≤ r, where r is the max-norm hyperparameter and\n",
    "∥ · ∥2 is the ℓ2 norm.\n",
    "\n",
    "Reducing r increases the amount of regularization and helps reduce overfitting. Maxnorm\n",
    "regularization can also help alleviate the vanishing/exploding gradients problems\n",
    "(if you are not using Batch Normalization).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "id": "g1-0gs3r7kgf",
    "outputId": "362b25ae-0510-442f-d96c-fff6b772aa6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Validation accuracy: 0.9542\n",
      "1 Validation accuracy: 0.968\n",
      "2 Validation accuracy: 0.974\n",
      "3 Validation accuracy: 0.9786\n",
      "4 Validation accuracy: 0.9784\n",
      "5 Validation accuracy: 0.977\n",
      "6 Validation accuracy: 0.9784\n",
      "7 Validation accuracy: 0.9804\n",
      "8 Validation accuracy: 0.9818\n",
      "9 Validation accuracy: 0.9822\n",
      "10 Validation accuracy: 0.9834\n",
      "11 Validation accuracy: 0.9834\n",
      "12 Validation accuracy: 0.9828\n",
      "13 Validation accuracy: 0.9832\n",
      "14 Validation accuracy: 0.9844\n",
      "15 Validation accuracy: 0.9834\n",
      "16 Validation accuracy: 0.9848\n",
      "17 Validation accuracy: 0.9846\n",
      "18 Validation accuracy: 0.9846\n",
      "19 Validation accuracy: 0.9838\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "def max_norm_regularizer(threshold, axes=1, name=\"max_norm\",\n",
    "                         collection=\"max_norm\"):\n",
    "    def max_norm(weights):\n",
    "        clipped = tf.clip_by_norm(weights, clip_norm=threshold, axes=axes)\n",
    "        clip_weights = tf.assign(weights, clipped, name=name)\n",
    "        tf.add_to_collection(collection, clip_weights)\n",
    "        return None # there is no regularization loss term\n",
    "    return max_norm\n",
    "  \n",
    "  \n",
    "# Then you can call this function to get a max norm regularizer\n",
    "# (with the threshold you want). When you create a hidden layer, you can pass this regularizer to the kernel_regularizer argument:  \n",
    "\n",
    "\n",
    "max_norm_reg = max_norm_regularizer(threshold=1.0)\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu,\n",
    "                              kernel_regularizer=max_norm_reg, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu,\n",
    "                              kernel_regularizer=max_norm_reg, name=\"hidden2\")\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n",
    "    \n",
    "    \n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum)\n",
    "    training_op = optimizer.minimize(loss)    \n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "\n",
    "n_epochs = 20\n",
    "batch_size = 50\n",
    "\n",
    "clip_all_weights = tf.get_collection(\"max_norm\")\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "            sess.run(clip_all_weights)\n",
    "        acc_valid = accuracy.eval(feed_dict={X: X_valid, y: y_valid}) \n",
    "        print(epoch, \"Validation accuracy:\", acc_valid)               \n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
